{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WClw_VPOmwXd"
   },
   "source": [
    "<img src=\"../../Img/backdrop-wh.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "# Topic Modeling\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Understand topic modeling and how it can be used to find themes and topics across posts.\n",
    "* Visualize topic models to facilitate exploration.\n",
    "* Evaluate and improve topic models using several methods.\n",
    "* Give names to topics, and use them to classify text.\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "üí≠ **Reflection**: Reflecting on ethical implications, biases, and social impact in data science.<br>\n",
    "\n",
    "### Sections\n",
    "1. [Topic Modeling](#topic)\n",
    "2. [Visualizing a Topic Model](#vis)\n",
    "3. [Tweaking the Data](#data)\n",
    "4. [Tweaking Hyperparameters](#hyper)\n",
    "5. [Calculating Topic Coherence](#coh)\n",
    "6. [Changing Amount of Topics](#topics)\n",
    "7. [Using Topic Models](#use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6EAS2bBZWyR"
   },
   "source": [
    "<a id='topic'></a>\n",
    "\n",
    "# Topic modeling\n",
    "\n",
    "This notebook introduces topic modeling. Topic modeling is a type of statistical modeling for the discovery of abstract \"topics\" that occur in a collection of documents. It is frequently used in NLP to aid the discovery of hidden semantic structures in a collection of texts.\n",
    "\n",
    "Before you start, please read [this post](https://tomvannuenen.medium.com/analyzing-reddit-communities-with-python-part-5-topic-modeling-a5b0d119add) for an explainer of how topic modeling (and LDA, which is just one form of topic modeling) works.\n",
    "\n",
    "We'll use the `Gensim` package to create our topic models, which also allows us to run tests to optimize our topic amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ib68CU9JeGgc"
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2574,
     "status": "ok",
     "timestamp": 1639939141559,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "jlWzV3rveIdK",
    "outputId": "f6f0d10b-b6ff-4236-aaa4-6ec1cc29e980"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../data/aita_pp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split up the lemmas--we need them split up to use in our topic model. All we need to do is run `.split()` on our \"pp_text\" column to tokenize the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JitcKXJqffVc"
   },
   "outputs": [],
   "source": [
    "lemmas_split = [lemma.split() for lemma in df['pp_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1639908506458,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "BBoCdE99fo61",
    "outputId": "881ec95c-f21a-4d8f-87fc-eb10642118ed"
   },
   "outputs": [],
   "source": [
    "lemmas_split[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNmgAQEoZWzx"
   },
   "source": [
    "## Creating a `Dictionary` with Gensim\n",
    "\n",
    "Now, let's create our gensim dictionary - a mapping of each word to a unique id ‚Äì a Document-Term matrix ‚Äì much like the `CountVectorizer` we saw last week. We'll use gensim's `Dictionary` class for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5U9p6rrZWzy"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Create Dictionary \n",
    "dictionary = corpora.Dictionary(lemmas_split)\n",
    "\n",
    "# filter extremes and assign new ids\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "dictionary.compactify() \n",
    "\n",
    "# SAVE DICT\n",
    "dictionary.save('../../data/aita_lda.dict')\n",
    "\n",
    "# Create Document-Term Matrix of our whole corpus \n",
    "corpus = [dictionary.doc2bow(text) for text in lemmas_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8U9L6iKDe22R"
   },
   "source": [
    "Topic modeling uses a **bag-of-words** model to represent documents in a corpus. In the bag-of-words model, a document is represented by word counts. Additional information, such as word order, is discarded.\n",
    "\n",
    "Let's view some of the corpus we have now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 172,
     "status": "ok",
     "timestamp": 1639858040767,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "1137g7AVdZDG",
    "outputId": "c5bc67b6-c8b9-4e42-ae44-cf9742c9485e"
   },
   "outputs": [],
   "source": [
    "corpus[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXRXTfMgZW0A"
   },
   "source": [
    "Observe the first 10 tuples above. Each consists of words with a unique id. This a mapping of (word_id, word_frequency). For example, (0, 1) above demonstrates that word id 0 occurs once in the first document. Word id 6 occurs 6 times, and so on. This is used as the input by the LDA model.\n",
    "\n",
    "If you want to see what word a given id corresponds to, pass the id as a key to the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1639822605042,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "0pKC0MBiZW0A",
    "outputId": "d65a3087-d363-4614-d561-16f65bd6f267"
   },
   "outputs": [],
   "source": [
    "dictionary[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNr34ffPZW0D"
   },
   "source": [
    "And if you want to see the associated id for some word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 525,
     "status": "ok",
     "timestamp": 1639779996499,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "dNhZ-fZ4ZW0D",
    "outputId": "5f3719d7-001b-42f3-8a18-1f7725bd0b24"
   },
   "outputs": [],
   "source": [
    "dictionary.token2id['father']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uep-_3ImZW0S"
   },
   "source": [
    "## Running a model\n",
    "\n",
    "Let's run our first Gensim LDA topic model! Check out the comments to learn about the function arguments we're using.\n",
    "\n",
    "Note that topic modeling essentially adds a third latent layer on top of the documents and tokens (which is the representation we saw last week when running SKLearn's `CountVectorizer()`. That third layer consists of topics. Topic modeling assumes that documents are made of topics, and topics is made up of tokens. It also assumes a Dirichlet probability distribution, which encourages docs to only consist of a handful of topics and topics only of a handful of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28047,
     "status": "ok",
     "timestamp": 1639913487603,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "GV46eCt2ydXL",
    "outputId": "5af4d56d-7564-40a8-d4ec-739f9778ce9c"
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "%time\n",
    "lda_model = LdaModel(corpus=corpus,   # stream of document vectors or sparse matrix of shape\n",
    "            id2word=dictionary,       # mapping from word IDs to words (for determining vocab size)\n",
    "            num_topics=10,            # amount of topics\n",
    "            random_state=100,         # seed to generate random state; useful for reproducibility\n",
    "            passes=2,                 # amount of iterations/epochs \n",
    "            per_word_topics=False)    # computing most-likely topics for each word "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAi5A0nFh1NS"
   },
   "source": [
    "The most challenging part about topic modeling is creating a *good*, i.e. interpretable, topic model. This is a heavily iterative process. The first thing we should do is visualize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vis'></a>\n",
    "\n",
    "# Visualizing a Topic Model\n",
    "\n",
    "One of the best ways to visualize a topic model is through the pyLDAvis package. `pyLDAvis` was designed to help users interpret the topics in a topic model. Let's start by downloading the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 992
    },
    "executionInfo": {
     "elapsed": 33843,
     "status": "ok",
     "timestamp": 1639933951198,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "RiccfvG7IuRZ",
    "outputId": "e02afe24-c09f-4b39-ee1e-5d93d3f0fbfb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pyLDAvis\n",
    "except ImportError:\n",
    "    !pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWRO12UXZW0d"
   },
   "source": [
    "PyLDAvis allows us to visualize our topics. A \"good\" topic model produces non-overlapping, fairly large bubbles, which should be scattered throughout the chart instead of being clustered in one quadrant. A model with too many topics will typically have many overlaps, small sized bubbles clustered in one region of the chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 916
    },
    "executionInfo": {
     "elapsed": 10987,
     "status": "ok",
     "timestamp": 1639940889440,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "Aze6E-0ZZW0e",
    "outputId": "81b759c9-9655-421f-d6e7-0648a7a42802"
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# feed the LDA model into the pyLDAvis instance\n",
    "lda_viz = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "lda_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOGnQ-WiZW0h"
   },
   "source": [
    "On the left, there is a 2D plot of the \"distance\" between all of the topics (labeled as the Intertopic Distance Map). This plot uses a multidimensional scaling (MDS) algorithm. \n",
    "- Similar topics should appear close together on the plot; dissimilar topics should appear far apart. \n",
    "- The relative size of a topic's circle in the plot corresponds to the relative frequency of the topic in the corpus.\n",
    "\n",
    "### Exploring topics and words\n",
    "- You can scrutinize a topic more closely by clicking on its circle, or entering its number in the \"selected topic\" box in the upper-left (Note that, though the data used by gensim and pyLDAvis are the same, they don't use the same ID numbers for topics.)\n",
    "- If you roll your mouse over a term in the bar chart on the right, the topic circles will resize in the plot on the left. This shows the strength of the relationship between the topics and the selected term.\n",
    "\n",
    "### Salience\n",
    "On the right, there is a bar chart with the top terms. When no topic is selected in the plot on the left, the bar chart shows the top-30 most **salient** terms in the corpus. A term's saliency is a measure of both how frequent the term is in the corpus and how \"distinctive\" it is in distinguishing between different topics.\n",
    "\n",
    "### Probability Vs Exclusivity \n",
    "When you select a particular topic, this bar chart changes to show the top-30 most \"relevant\" terms for the selected topic. The relevance metric is controlled by the parameter Œª, which can be adjusted with a slider above the bar chart:\n",
    "\n",
    "* Setting Œª close to 1.0 (the default) will rank the terms according to their probability within the topic.\n",
    "* Setting Œª close to 0.0 will rank the terms according to their \"distinctiveness\" or \"exclusivity\" within the topic. This means that terms that occur only in this topic, and do not occur in other topics.\n",
    "\n",
    "You can move the slider between 0.0 and 1.0 to weigh term probability and exclusivity.\n",
    "\n",
    "### Exploring the graph\n",
    "The interactive visualization pyLDAvis produces is helpful for **individual** topics: you can manually select each topic to view its top most frequent and/or \"relevant\" terms, using different values of the Œª parameter. This can help when you're trying to assign a name or \"meaning\" to each topic. \n",
    "\n",
    "It also helps you to see the **relationships** between topics: exploring the Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics.\n",
    "\n",
    "### Getting insights about the model\n",
    "As you can see, this model probably has too many topics: they are overlapping, and most of them appear in one corner of the graph. So we have our first hint that we might want to alter our model. Let's start by tweaking our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwWnkvExc11X"
   },
   "source": [
    "<a id='data'></a>\n",
    "\n",
    "# Tweaking the data\n",
    "\n",
    "Remember we used the lemmas of our dataset? What if we tweaked it some more -- for instance, by POS tagging?\n",
    "\n",
    "POS - Part of Speech - tagging is the process of marking up a word in a corpus to a corresponding part of a speech tag (noun, adjective, verb, etc.). Often it is a process of converting a sentence to a list of tuples where each tuple takes the form of (word, tag).\n",
    "\n",
    "Let's do this with spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZtZ7YtScsMX"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "import spacy\n",
    "#!spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def POS(text, allowed_postags = ['NOUN', 'ADJ']):\n",
    "    parsed = nlp(text)\n",
    "    return [token.lemma_ for token in parsed if token.pos_ in allowed_postags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a long time\n",
    "pos_lemmas_split = [POS(text) for text in tqdm(df['pp_text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn our POS tagged lemmas into a string so we can save them in our DF\n",
    "str_pos_lemmas = [' '.join(t) for t in pos_lemmas_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_pos_lemmas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos_text'] = str_pos_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create new dictionary and corpus objects for Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary \n",
    "pos_dictionary = corpora.Dictionary(tqdm(pos_lemmas_split))\n",
    "\n",
    "# filter extremes and assign new ids\n",
    "pos_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "pos_dictionary.compactify() \n",
    "\n",
    "# SAVE DICT\n",
    "pos_dictionary.save('../../data/aita_pos_lda.dict')\n",
    "\n",
    "# Create Document-Term Matrix of our whole corpus \n",
    "pos_corpus = [pos_dictionary.doc2bow(text) for text in tqdm(pos_lemmas_split)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sP9L4KPXyXm"
   },
   "source": [
    "<a id='hyper'></a>\n",
    "\n",
    "# Tweaking hyperparameters\n",
    "\n",
    "Next, let's change some hyperparameters. This can also determine how interpretable our topic models become.\n",
    "\n",
    "`passes` controls how often we train the model on the entire corpus. Another word for passes might be ‚Äúepochs‚Äù. It defaults to `1` but we might want to set it to a higher number.\n",
    " \n",
    "Gensim's designer suggests the following way to choose the amount of passes. First, enable `logging`, and set `eval_every = 1` in `LdaModel`. This will yield a **perplexity** score for every update. Perplexity is a measure of how well a probability model predicts a sample. It captures how surprised a model is of new data it has not seen before, and is measured as the normalized log-likelihood of a held-out test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufxXZDJMYKB2"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(filename='../../data/gensim.log', filemode='w', format=\"%(asctime)s:%(levelname)s:%(message)s\", level=logging.INFO)\n",
    "\n",
    "lda_model_tweak = LdaModel(corpus=pos_corpus,\n",
    "                           id2word=pos_dictionary,\n",
    "                           num_topics=20, \n",
    "                           random_state=100,\n",
    "                           eval_every=1,           # show perplexity after every update for visualization\n",
    "                           passes=5,              # number of model training cycles, aka epochs\n",
    "                           per_word_topics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jgq0gBoOTNha"
   },
   "source": [
    "Using regular expressions we can now search through our newly created \"gensim.log\" file and find  / plot the relevant information.\n",
    "This shows how topic/word assignments reach a steady state and no longer change much, (i.e. converge). Adding \"passes\" when running the topic model can increase the log-likelihood. The higher the value of the log-likelihood, the better our model fits the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "\n",
    "p = re.compile(r\"(-*\\d+\\.\\d+) per-word .* (\\d+\\.\\d+) perplexity\")\n",
    "matches = [p.findall(l) for l in open('../../data/gensim.log')]\n",
    "matches = [m for m in matches if len(m) > 0]\n",
    "tuples = [t[0] for t in matches]\n",
    "likelihood = [float(t[0]) for t in tuples]\n",
    "perplexity = [float(t[1]) for t in tuples]\n",
    "iter = list(range(0,len(tuples)*10,10))\n",
    "plt.plot(iter,likelihood,c=\"black\")\n",
    "plt.ylabel(\"log likelihood\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.title(\"Topic Model Convergence\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that the graph contains about 5 \"peaks\", which refers to the number of `passes` we set above. As you can see, the model converges quite rapidly, so we do not need to set `passes` very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osn95IYrZW0i"
   },
   "source": [
    "<a id='coh'></a>\n",
    "\n",
    "# Calculating Topic Coherence\n",
    "\n",
    "We can also apply statistical measures to help us determine the optimal number of topics in our topic model.\n",
    "\n",
    "**Topic Coherence** measures the score of a single topic by measuring the degree of semantic similarity between high scoring words in the topic. This helps to distinguish between topics that are semantically interpretable topics, and topics that are artifacts of statistical inference. \n",
    "\n",
    "A set of statements or facts is said to be coherent if the statements support each other. An example of a coherent fact set is ‚Äúthe game is a team sport‚Äù, ‚Äúthe game is played with a ball‚Äù, ‚Äúthe game demands great physical efforts‚Äù\n",
    "\n",
    "A good model will generate topics with *high* topic coherence scores. Good topics are topics that can be described by a short label, therefore this is what the topic coherence measure should capture.\n",
    "\n",
    "üí°**Tip**: There are different ways to measure coherence. For instance, the c_v measure used here is calculated based on a combination of confirmation measures, such as how often word pairs occur together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43064,
     "status": "ok",
     "timestamp": 1639913530642,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "wQFU0eNUZW0i",
    "outputId": "9de0d9f7-993b-4228-b6a7-0bbc8e25ffd5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import logging\n",
    "#logging.getLogger().setLevel(logging.CRITICAL)\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model = CoherenceModel(model=lda_model_tweak, corpus=pos_corpus, texts=tqdm(pos_lemmas_split), dictionary=pos_dictionary, coherence='c_v') \n",
    "coherence = coherence_model.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xg-5r0htZW0k"
   },
   "source": [
    "There's no hard or fast rule on what makes a good coherence score.\n",
    "In general, a coherence score of .4 means you probably are not using the right number of topics. .6 to .7 is good. Anything more is suspiciously great. As you can see, our coherence score here is very low, so we should definitely try to improve upon our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjFdiMk6ZW0v"
   },
   "source": [
    "<a id='topics'></a>\n",
    "# Changing number of topics\n",
    "\n",
    "The final and most important thing we can do to find optimal scores is to play around with the amount of topics our model creates. One way to do this is to build many LDA models with different values of number of topics (k), and then pick the one that gives the highest coherence value. Choosing a ‚Äòk‚Äô at the end of a rapid growth of topic coherence usually yields meaningful and interpretable topics. If you see the same keywords being repeated in multiple topics, it‚Äôs probably a sign that the ‚Äòk‚Äô is too large.\n",
    "\n",
    "This `compute_coherence_values()` function trains multiple LDA models, provides the models, and tells you their corresponding coherence scores.\n",
    "\n",
    "Also note the docstring I create here: these are documentation for the functions we create. It describes what a function does, and can be called using `help(function_X)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOZRa_4LZW0w"
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, start, limit, step):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : Tokenized text (list of lis of str)\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    total_amount = (limit - start - 1) // step + 1\n",
    "    current_amount = 0\n",
    "    passes = 5\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=100, \n",
    "                         update_every=1, passes=passes, alpha='auto', per_word_topics=False)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        # When using 'c_v' texts should be provided, corpus isn‚Äôt needed. \n",
    "        # When using ‚Äòu_mass‚Äô corpus should be provided, if texts is provided, it will be converted to corpus using the dictionary \n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        current_amount += 1\n",
    "        print(\"Built \" + str(current_amount) + \" of \" + str(total_amount) + \" models\")\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eX4Mc6JZo4sG"
   },
   "source": [
    "Using our new function, let's run a bunch of topic models with different amounts of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 242071,
     "status": "ok",
     "timestamp": 1639940656827,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "LYWcr2NhZW0z",
    "outputId": "0f0f4a01-b509-4c46-d11d-015179e38522"
   },
   "outputs": [],
   "source": [
    "# Can take a long time to run\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=pos_dictionary, \n",
    "                                                        corpus=pos_corpus, texts=pos_lemmas_split, \n",
    "                                                        start=6, limit=30, step=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks6jTrgtpC1m"
   },
   "source": [
    "Now, from all those models, let's visualize the output of the coherence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 901,
     "status": "ok",
     "timestamp": 1639940737397,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "td7Zi7YpZW03",
    "outputId": "dee7e449-79a6-461b-c582-dba9eeb9cef6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Show graph\n",
    "start=6; limit=27; step=5\n",
    "x = range(start, limit, step) # range uses start, stop, and incrementation\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 548,
     "status": "ok",
     "timestamp": 1639940764419,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "2zX73ICjZW05",
    "outputId": "e1593fa6-dcd7-4bf9-eb75-5b838e238b10",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print these coherence scores\n",
    "c = 0\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(f\"model_list[{c}]: Num Topics = {m}, Coherence Value = {round(cv, 4)}\")\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPwvgE04ZW0_"
   },
   "source": [
    "If the coherence score seems to keep increasing, it generally makes sense to pick the model that gave the highest CV before dropping again. Following this \"elbow method\", we have a few options.\n",
    "\n",
    "However, these methods are only heuristics, and the scores we have here are not that far apart. At this point you should *go back* to pyLDAvis using the models from our `model_list`, and compare them to see which topic model looks better. Based on these combined insights, I will pick the model with 11 topics.\n",
    "\n",
    "Note that this is **not** the model with the highest coherence value! While these metrics can be useful, they should never be followed blindly. What matters more is the **interpretability** of topic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzmroqZHm3M3"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# SAVE MODEL\n",
    "optimal_lda_model = model_list[1]\n",
    "optimal_lda_model.save('../../data/aita_pos_lda_optimal.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# feed the LDA model into the pyLDAvis instance\n",
    "lda_viz = gensimvis.prepare(optimal_lda_model, pos_corpus, pos_dictionary)\n",
    "lda_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to load these models from disk again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD MODEL\n",
    "optimal_lda_model = LdaModel.load('../../data/aita_pos_lda_optimal.model')\n",
    "\n",
    "# LOAD DICT\n",
    "pos_dictionary = corpora.Dictionary.load('../../data/aita_pos_lda.dict')\n",
    "\n",
    "# LOAD CORPUS\n",
    "pos_corpus = [pos_dictionary.doc2bow(text) for text in tqdm(pos_lemmas_split)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming our topics\n",
    "\n",
    "The next thing we should do is name our topics. This is the most important interpretative step in the process: after all, our model has no semantic knowledge of the data. We will print out the top words of each topic, then go over all of them and give them names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1639940904382,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "h3IDZcmfZW1A",
    "outputId": "9eff0a15-579a-4d68-afb7-e29d7f3d6bb8"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Select the ideal model and print the topics\n",
    "model_topics = optimal_lda_model.show_topics(formatted=False)\n",
    "pprint(optimal_lda_model.print_topics(num_words=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: If you are using your own data, make sure to replace the following names with those of your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWep2BUPAc97"
   },
   "outputs": [],
   "source": [
    "# giving names to our topics\n",
    "topic_names = {0: 'family, kids', \n",
    "               1: 'married life, neighborhoods, pets',\n",
    "               2: 'school, work, cars',\n",
    "               3: 'family, siblings',\n",
    "               4: 'dating, friends',\n",
    "               5: 'lifestyle, looks',\n",
    "               6: 'work',\n",
    "               7: 'money, work',\n",
    "               8: 'family, babies',\n",
    "               9: 'food',\n",
    "               10: 'sleep'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naming topics is a heavily iterative process, based on looking closer at particular posts (see below).\n",
    "\n",
    "For instance, I had initially named topic 1 \"weddings\", but after reading some posts with this dominant topic, I renamed it to \"married life, neighborhoods, pets\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fK6uo1EZW1M"
   },
   "source": [
    "<a id='use'></a>\n",
    "\n",
    "# Using Topic Models\n",
    "\n",
    "Topic modeling has several practical applications. One of them is to determine what topic a Reddit post is about. To figure this out, we find the topic number that has the highest percentage contribution to that thread. We'll write a `dominant_topic()` function that aggregates this information in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "elapsed": 118204,
     "status": "ok",
     "timestamp": 1639923042784,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "iz_TBN_dZW1Q",
    "outputId": "49fef3dd-409f-444e-dc75-fd443d24ca0f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dominant_topic(ldamodel=optimal_lda_model, corpus=corpus, texts=df['selftext']):\n",
    "    # Create DF\n",
    "    topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each thread\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                # get value of topic_names dict based on key\n",
    "                topic_name = topic_names[topic_num]\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                new_row = pd.DataFrame([[int(topic_num), topic_name, round(prop_topic,4), topic_keywords]])\n",
    "                topics_df = pd.concat([topics_df, new_row], ignore_index=True)\n",
    "\n",
    "            else:\n",
    "                break\n",
    "    topics_df.columns = ['Dominant_Topic', 'Dominant_Topic_Name', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    topics_df = pd.concat([topics_df, contents], axis=1)\n",
    "    return topics_df \n",
    "\n",
    "# Run function\n",
    "df_topic_keywords = dominant_topic(ldamodel=optimal_lda_model, corpus=pos_corpus, texts=df['selftext'])\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_keywords.reset_index(drop=True)\n",
    "df_dominant_topic.columns = ['Dominant_Topic', 'Dominant_Topic_Name', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_OLtA9Wtq2F"
   },
   "source": [
    "We can now find the posts with a dominant topic using `.loc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1639923059054,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "bQBSqG5QtSud",
    "outputId": "ed2a055a-ca20-4e9d-9717-7a13000ab208"
   },
   "outputs": [],
   "source": [
    "df_dominant_topic.loc[df_dominant_topic['Dominant_Topic_Name'] == 'family, kids']['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.loc[df_dominant_topic['Dominant_Topic_Name'] == 'family, kids']['Text'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That does look to be about family life, and particularly about having kids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding topics to original DF\n",
    "\n",
    "Once we are happy with your topic names, we can add the dominant topic names to our original DataFrame and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dom_topic'] = df_dominant_topic['Dominant_Topic_Name']\n",
    "df['dom_topic_num'] = df_dominant_topic['Dominant_Topic']\n",
    "\n",
    "df.to_csv('../../data/aita_lda.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5t8WVkZ2134h"
   },
   "source": [
    "# üí≠ Reflection: The hermeneutics of topic modeling \n",
    "\n",
    "One thought to end with: for most topic models you will create, it will be hard to apply a meaningful interpretation to each topic. Not every topic will have some meaningful insight \"fall out of it\" upon first inspection. This is a typical issue in machine learning, which can pick up on patterns that might not make sense to humans.\n",
    "\n",
    "It is an open question to which extent you should let yourself be surprised by particular combinations of words in a topic, or if topic models primarily should follow the intuitions you already have as a researcher. What makes for a \"good\" topic model probably straddles the boundaries of surprise and expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Key Points\n",
    "\n",
    "* Topic modeling can help us find themes and topics in textual data.\n",
    "* Topic models can be evaluated and improved based on coherence metrics; however, using your eyes to see whether topics make intuitive sense is just as important.\n",
    "* Topic models yield information that can be used to do different things, such as finding submissions with particular topics, or classifying texts.\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week 3-1 Topic Modeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dlab",
   "language": "python",
   "name": "dlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
